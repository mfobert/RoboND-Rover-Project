{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rover Project Test Notebook\n",
    "This notebook contains the functions from the lesson and provides the scaffolding you need to test out your mapping methods.  The steps you need to complete in this notebook for the project are the following:\n",
    "\n",
    "* Write new functions (or modify existing ones) to report and map out detections of obstacles and rock samples (yellow rocks)\n",
    "* Populate the `process_image()` function with the appropriate steps/functions to go from a raw image to a worldmap.\n",
    "* Run the cell that calls `process_image()` using `moviepy` functions to create video output\n",
    "* Once you have mapping working, move on to modifying `perception.py` and `decision.py` to allow your rover to navigate and map in autonomous mode!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports/Notebook Config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Make some of the relevant imports\n",
    "import cv2 # OpenCV for perspective transform\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc # For saving images as needed\n",
    "import glob  # For reading in a list of images from a folder\n",
    "import imageio\n",
    "imageio.plugins.ffmpeg.download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Look at the Data\n",
    "Next, read in and display a random image from the test dataset folder. <b>Note</b>, you must change this path to your test dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "outputExpanded": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '../../recorded-data/IMG/*'\n",
    "\n",
    "img_list = glob.glob(path)\n",
    "idx = np.random.randint(0, len(img_list)-1)\n",
    "image = mpimg.imread(img_list[idx])\n",
    "plt.imshow(image)\n",
    "plt.title('random test image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Our Calibration Model\n",
    "Read in and display example grid and rock sample calibration images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_grid = '../calibration_images/grid.jpg'\n",
    "example_rock = '../calibration_images/rock_2.jpg'\n",
    "grid_img = mpimg.imread(example_grid)\n",
    "rock_img = mpimg.imread(example_rock)\n",
    "\n",
    "fig = plt.figure(figsize=(12,3))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Grid Defintion\")\n",
    "plt.imshow(grid_img)\n",
    "plt.subplot(122)\n",
    "plt.imshow(rock_img)\n",
    "plt.title(\"Rock Defintion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, get grid points for transform from interactive matplotlib graph:\n",
    "\n",
    "From the below graph we get:\n",
    "\n",
    "BL: [6.7,145.5] <br>\n",
    "BR: [306.1, 142.7]<br>\n",
    "TR: [ 197.7, 96.8]<br>\n",
    "TL: [118.2, 96.7]<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook #make interactive\n",
    "%matplotlib inline\n",
    "plt.imshow(grid_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the perspective transform function and test it on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def perspect_transform(img, src, dst):\n",
    "           \n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    warped = cv2.warpPerspective(img, M, (img.shape[1], img.shape[0]))# keep same size as input image\n",
    "    \n",
    "    return warped\n",
    "\n",
    "\n",
    "# We are mapping from 1 meter squared boxes in the rovers perspective to\n",
    "# a 10x10 pixel box in the top-down view space\n",
    "dst_size = 10 \n",
    "# The rover is slightly offset from the actual image. Account with bottom_offset:\n",
    "bottom_offset = 6\n",
    "\n",
    "\n",
    "#Source values gathered above\n",
    "source = np.float32([[6.7,145.5], \n",
    "                     [306.1, 142.7],\n",
    "                     [ 197.7, 96.8], \n",
    "                     [118.2, 96.7]])\n",
    "\n",
    "#map rover perspective to bottom middle of top down view:\n",
    "img_w = image.shape[1]\n",
    "img_h = image.shape[0]\n",
    "\n",
    "destination = np.float32([[img_w/2 - dst_size/2, img_h - bottom_offset],\n",
    "                  [img_w/2 + dst_size/2, img_h - bottom_offset],\n",
    "                  [img_w/2 + dst_size/2, img_h - dst_size - bottom_offset], \n",
    "                  [img_w/2 - dst_size/2, img_h - dst_size - bottom_offset],\n",
    "                  ])\n",
    "\n",
    "warped = perspect_transform(grid_img, source, destination)\n",
    "plt.imshow(warped)\n",
    "plt.title(\"Top down rover-warped perspective\")\n",
    "#scipy.misc.imsave('../output/warped_example.jpg', warped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Thresholding\n",
    "Define the color thresholding function, find rocks and obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify pixels above the threshold\n",
    "# Threshold of RGB > 160 does a nice job of identifying ground pixels only\n",
    "def bw_thresh(img, bw_threshold_value = 160):\n",
    "    \n",
    "    #convert to black and white with opencv\n",
    "    img_bw = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Create an array of zeros same xy size as img\n",
    "    color_select = np.zeros_like(img)\n",
    "    \n",
    "    #mask based on threshold\n",
    "    mask = img_bw >= bw_threshold_value\n",
    "    \n",
    "    # Index the array of zeros with the boolean array and set to 1\n",
    "    color_select[mask] = [1,1,1]\n",
    "    color_select[~mask] = [0,0,0]\n",
    "    # Return the binary image\n",
    "    return color_select\n",
    "\n",
    "#cut out sky\n",
    "def cut_top_of_colored_image(img, pixels_to_cut=60):\n",
    "    cut_img = np.copy(img)\n",
    "    cut_img[:pixels_to_cut,:] = [0,0,0]\n",
    "    return cut_img\n",
    "\n",
    "def rock_thresh(img):\n",
    "    rocks = np.copy(img)\n",
    "    \n",
    "    #filter out walls\n",
    "    mask = bw_thresh(img, 90) < 1\n",
    "    \n",
    "    rocks[mask] = 0\n",
    "    \n",
    "    #filter out ground  - use hsv values\n",
    "    rocks = cv2.cvtColor(rocks, cv2.COLOR_BGR2HSV)\n",
    "    mask = rocks[:,:,1] > 100\n",
    "    \n",
    "    rocks[~mask] = [0,0,0]\n",
    "    rocks[mask]=[1,1,1]\n",
    "    \n",
    "    return rocks\n",
    "\n",
    "test_image = np.copy(rock_img)\n",
    "\n",
    "#cut out the sky\n",
    "cut_img = cut_top_of_colored_image(test_image)\n",
    "\n",
    "plt.figure(figsize=(11,9))\n",
    "plt.subplot(3,2,1)\n",
    "plt.title(\"sky cut out\")\n",
    "plt.imshow(cut_img)\n",
    "\n",
    "#threshold to get navigable terrain\n",
    "threshed = bw_thresh(cut_img)\n",
    "plt.subplot(3,2,2)\n",
    "plt.title(\"thresholded - navigable terrain\")\n",
    "plt.imshow(threshed[:,:,0], cmap = \"gray\")\n",
    "\n",
    "#warp cut image\n",
    "warped = perspect_transform(threshed, source, destination)\n",
    "plt.subplot(3,2,3)\n",
    "plt.title(\"warped\")\n",
    "plt.imshow(warped[:,:,0], cmap = \"gray\")\n",
    "\n",
    "#get rocks\n",
    "rocks_rover_perspect = rock_thresh(cut_img)\n",
    "plt.subplot(3,2,4)\n",
    "plt.title(\"rocks\")\n",
    "plt.imshow(rocks_rover_perspect[:,:,0], cmap = \"gray\")\n",
    "\n",
    "warped_rocks = perspect_transform(rocks_rover_perspect, source, destination)\n",
    "plt.subplot(3,2,5)\n",
    "plt.title(\"warped rocks \")\n",
    "plt.imshow(warped_rocks[:,:,0], cmap = \"gray\")\n",
    "\n",
    "\n",
    "#scipy.misc.imsave('../output/warped_threshed.jpg', threshed*255)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Transformations\n",
    "Define the functions used to do coordinate transforms and apply them to an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "# Define a function to convert from image coords to rover coords\n",
    "def rover_coords(binary_img):\n",
    "    # Identify nonzero pixels\n",
    "    ypos, xpos = binary_img.nonzero()\n",
    "    # Calculate pixel positions with reference to the rover position being at the \n",
    "    # center bottom of the image.  \n",
    "    x_pixel = -(ypos - binary_img.shape[0]).astype(np.float)\n",
    "    y_pixel = -(xpos - binary_img.shape[1]/2 ).astype(np.float)\n",
    "    return x_pixel, y_pixel\n",
    "\n",
    "# Define a function to convert to radial coords in rover space\n",
    "def to_polar_coords(x_pixel, y_pixel):\n",
    "    # Convert (x_pixel, y_pixel) to (distance, angle) \n",
    "    # in polar coordinates in rover space\n",
    "    # Calculate distance to each pixel\n",
    "    dist = np.sqrt(x_pixel**2 + y_pixel**2)\n",
    "    # Calculate angle away from vertical for each pixel\n",
    "    angles = np.arctan2(y_pixel, x_pixel)\n",
    "    return dist, angles\n",
    "\n",
    "# Define a function to map rover space pixels to world space\n",
    "def rotate_pix(xpix, ypix, yaw):\n",
    "    # Convert yaw to radians\n",
    "    yaw_rad = yaw * np.pi / 180\n",
    "    xpix_rotated = (xpix * np.cos(yaw_rad)) - (ypix * np.sin(yaw_rad))\n",
    "                            \n",
    "    ypix_rotated = (xpix * np.sin(yaw_rad)) + (ypix * np.cos(yaw_rad))\n",
    "    # Return the result  \n",
    "    return xpix_rotated, ypix_rotated\n",
    "\n",
    "def translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale): \n",
    "    # Apply a scaling and a translation\n",
    "    xpix_translated = (xpix_rot / scale) + xpos\n",
    "    ypix_translated = (ypix_rot / scale) + ypos\n",
    "    # Return the result  \n",
    "    return xpix_translated, ypix_translated\n",
    "\n",
    "\n",
    "# Define a function to apply rotation and translation (and clipping)\n",
    "# Once you define the two functions above this function should work\n",
    "def pix_to_world(xpix, ypix, xpos, ypos, yaw, world_size, scale):\n",
    "    # Apply rotation\n",
    "    xpix_rot, ypix_rot = rotate_pix(xpix, ypix, yaw)\n",
    "    # Apply translation\n",
    "    xpix_tran, ypix_tran = translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale)\n",
    "    # Perform rotation, translation and clipping all at once\n",
    "    x_pix_world = np.clip(np.int_(xpix_tran), 0, world_size - 1)\n",
    "    y_pix_world = np.clip(np.int_(ypix_tran), 0, world_size - 1)\n",
    "    # Return the result\n",
    "    return x_pix_world, y_pix_world\n",
    "\n",
    "# Grab another random image\n",
    "idx = np.random.randint(0, len(img_list)-1)\n",
    "image = mpimg.imread(img_list[idx])\n",
    "threshed = bw_thresh(image)\n",
    "warped = perspect_transform(threshed, source, destination)\n",
    "warped = warped[:,:,0]\n",
    "threshed = threshed[:,:,0]\n",
    "\n",
    "# Calculate pixel values in rover-centric coords and distance/angle to all pixels\n",
    "xpix, ypix = rover_coords(warped)\n",
    "dist, angles = to_polar_coords(xpix, ypix)\n",
    "mean_dir = np.mean(angles)\n",
    "\n",
    "# Do some plotting\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "plt.subplot(221)\n",
    "plt.title(\"test image\")\n",
    "plt.imshow(image)\n",
    "plt.subplot(222)\n",
    "plt.imshow(warped)\n",
    "plt.title(\"warped image\")\n",
    "plt.subplot(223)\n",
    "plt.imshow(threshed, cmap='gray')\n",
    "plt.title(\"thresholded warped image\")\n",
    "plt.subplot(224)\n",
    "plt.plot(xpix, ypix, '.')\n",
    "plt.title(\"Navigable terrain - rover centric\")\n",
    "plt.ylim(-160, 160)\n",
    "plt.xlim(0, 160)\n",
    "arrow_length = 100\n",
    "x_arrow = arrow_length * np.cos(mean_dir)\n",
    "y_arrow = arrow_length * np.sin(mean_dir)\n",
    "plt.arrow(0, 0, x_arrow, y_arrow, color='red', zorder=2, head_width=10, width=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rock_world_coordinates(img, source, destination,xpos,ypos,yaw,world_size,scale):\n",
    "    rover_perspect = np.copy(img)\n",
    "    rover_perspect= cut_top_of_colored_image(rover_perspect)\n",
    "    threshed = rock_thresh(rover_perspect)\n",
    "    top_down = perspect_transform(threshed, source, destination)\n",
    "    \n",
    "    top_down = top_down[:,:,0]\n",
    "    threshed = threshed[:,:,0]\n",
    "    xpix, ypix = rover_coords(top_down)\n",
    "    \n",
    "    \n",
    "    #filter distances too far\n",
    "    pix_mask = xpix**2+ypix**2 <= 80**2\n",
    "    xpix=xpix[pix_mask] \n",
    "    ypix=ypix[pix_mask] \n",
    "    \n",
    "    #filter distances angles outside of 15deg\n",
    "    pix_mask = (np.arctan2(ypix,xpix)*180/np.pi)**2 < 30**2\n",
    "    xpix=xpix[pix_mask] \n",
    "    ypix=ypix[pix_mask] \n",
    "    '''\n",
    "    plt.imshow(rover_perspect)\n",
    "    plt.show()\n",
    "    plt.imshow(threshed)\n",
    "    plt.show()\n",
    "    plt.imshow(top_down)\n",
    "    plt.show()\n",
    "    plt.plot(xpix,ypix, \".\")\n",
    "    plt.ylim(-160, 160)\n",
    "    plt.xlim(0, 160)\n",
    "    plt.show()\n",
    "    '''\n",
    "    \n",
    "    return pix_to_world(xpix,ypix,xpos,ypos,yaw,world_size,scale)\n",
    "\n",
    "def get_obstacle_world_coordinates(img, source, destination,xpos,ypos,yaw,world_size,scale):\n",
    "    threshed = bw_thresh(img)\n",
    "    warped = perspect_transform(threshed, source, destination)\n",
    "    warped = warped[:,:,0]\n",
    "    threshed = threshed[:,:,0]\n",
    "    mask = threshed > 0\n",
    "    threshed = np.ones_like(threshed)\n",
    "    threshed[mask]=0\n",
    "    threshed = perspect_transform(threshed, source, destination)\n",
    "    xpix, ypix = rover_coords(threshed)\n",
    "    pix_mask = xpix**2+ypix**2 >= 80**2\n",
    "    xpix[pix_mask] = 0\n",
    "    ypix[pix_mask] = 0\n",
    "    \n",
    "    #filter distances angles outside of 15deg\n",
    "    pix_mask = (np.arctan2(ypix,xpix)*180/np.pi)**2 > 30**2\n",
    "    xpix[pix_mask] = 0\n",
    "    ypix[pix_mask] = 0\n",
    "    '''\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    plt.imshow(threshed)\n",
    "    plt.show()\n",
    "    plt.imshow(warped)\n",
    "    plt.show()\n",
    "    plt.plot(xpix,ypix, \".\")\n",
    "    plt.ylim(-160, 160)\n",
    "    plt.xlim(0, 160)\n",
    "    plt.show()\n",
    "    '''\n",
    "    return pix_to_world(xpix,ypix,xpos,ypos,yaw,world_size,scale)\n",
    "\n",
    "def get_navigible_terrain_world_coordinates(img, source, destination,xpos,ypos,yaw,world_size,scale):\n",
    "    threshed = bw_thresh(cut_top_of_colored_image(img))\n",
    "    warped = perspect_transform(threshed, source, destination)\n",
    "    warped = warped[:,:,0]\n",
    "    threshed = threshed[:,:,0]\n",
    "    xpix, ypix = rover_coords(warped)\n",
    "    #filter distances too far\n",
    "    pix_mask = xpix**2+ypix**2 >= 80**2\n",
    "    xpix[pix_mask] = 0\n",
    "    ypix[pix_mask] = 0\n",
    "    \n",
    "    #filter distances angles outside of 15deg\n",
    "    pix_mask = (np.arctan2(ypix,xpix)*180/np.pi)**2 > 30**2\n",
    "    xpix[pix_mask] = 0\n",
    "    ypix[pix_mask] = 0\n",
    "    '''\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    plt.imshow(threshed)\n",
    "    plt.show()\n",
    "    plt.imshow(warped)\n",
    "    plt.show()\n",
    "    plt.plot(xpix,ypix, \".\")\n",
    "    plt.ylim(-160, 160)\n",
    "    plt.xlim(0, 160)\n",
    "    plt.show()\n",
    "    '''\n",
    "    xpix_w, ypix_w = pix_to_world(xpix,ypix,xpos,ypos,yaw,world_size,scale)\n",
    "    return xpix_w, ypix_w, threshed\n",
    "\n",
    "#get_obstacle_world_coordinates(rock_img,source, destination,0,0,0,0,0)\n",
    "#get_navigible_terrain_world_coordinates(rock_img,source, destination,0,0,0,0,0)\n",
    "get_rock_world_coordinates(rock_img,source, destination,0,0,0,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in saved data and ground truth map of the world\n",
    "The next cell is all setup to read your saved data into a `pandas` dataframe.  Here you'll also read in a \"ground truth\" map of the world, where white pixels (pixel value = 1) represent navigable terrain.  \n",
    "\n",
    "After that, we'll define a class to store telemetry data and pathnames to images.  When you instantiate this class (`data = Databucket()`) you'll have a global variable called `data` that you can refer to for telemetry and map data within the `process_image()` function in the following cell.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Import pandas and read in csv file as a dataframe\n",
    "import pandas as pd\n",
    "# Change the path below to your data directory\n",
    "# If you are in a locale (e.g., Europe) that uses ',' as the decimal separator\n",
    "# change the '.' to ','\n",
    "df = pd.read_csv('../../recorded-data/robot_log.csv', delimiter=';', decimal='.')\n",
    "csv_img_list = df[\"Path\"].tolist() # Create list of image pathnames\n",
    "# Read in ground truth map and create a 3-channel image with it\n",
    "ground_truth = mpimg.imread('../calibration_images/map_bw.png')\n",
    "ground_truth_3d = np.dstack((ground_truth*0, ground_truth*255, ground_truth*0)).astype(np.float)\n",
    "\n",
    "# Creating a class to be the data container\n",
    "# Will read in saved data from csv file and populate this object\n",
    "# Worldmap is instantiated as 200 x 200 grids corresponding \n",
    "# to a 200m x 200m space (same size as the ground truth map: 200 x 200 pixels)\n",
    "# This encompasses the full range of output position values in x and y from the sim\n",
    "class Databucket():\n",
    "    def __init__(self):\n",
    "        self.images = csv_img_list  \n",
    "        self.xpos = df[\"X_Position\"].values\n",
    "        self.ypos = df[\"Y_Position\"].values\n",
    "        self.yaw = df[\"Yaw\"].values\n",
    "        self.count = 0 # This will be a running index\n",
    "        self.worldmap = np.zeros((200, 200, 3)).astype(np.float)\n",
    "        self.ground_truth = ground_truth_3d # Ground truth worldmap\n",
    "\n",
    "# Instantiate a Databucket().. this will be a global variable/object\n",
    "# that you can refer to in the process_image() function below\n",
    "data = Databucket()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function to process stored images\n",
    "\n",
    "Modify the `process_image()` function below by adding in the perception step processes (functions defined above) to perform image analysis and mapping.  The following cell is all set up to use this `process_image()` function in conjunction with the `moviepy` video processing package to create a video from the images you saved taking data in the simulator.  \n",
    "\n",
    "In short, you will be passing individual images into `process_image()` and building up an image called `output_image` that will be stored as one frame of video.  You can make a mosaic of the various steps of your analysis process and add text as you like (example provided below).  \n",
    "\n",
    "\n",
    "\n",
    "To start with, you can simply run the next three cells to see what happens, but then go ahead and modify them such that the output video demonstrates your mapping process.  Feel free to get creative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to pass stored images to\n",
    "# reading rover position and yaw angle from csv file\n",
    "# This function will be used by moviepy to create an output video\n",
    "def process_image(img):\n",
    "    # Example of how to use the Databucket() object defined above\n",
    "    # to print the current x, y and yaw values \n",
    "    #print(data.xpos[data.count], data.ypos[data.count], data.yaw[data.count])\n",
    "\n",
    "    # TODO: \n",
    "    # 1) Define source and destination points for perspective transform\n",
    "        #edit later, should only be done once to find M\n",
    "    \n",
    "    dst_size = 10 \n",
    "    bottom_offset = 6\n",
    "\n",
    "    #Source values gathered above\n",
    "    source = np.float32([[6.7,145.5], \n",
    "                         [306.1, 142.7],\n",
    "                         [ 197.7, 96.8], \n",
    "                         [118.2, 96.7]])\n",
    "\n",
    "    #map rover perspective to bottom middle of top down view:\n",
    "    img_w = image.shape[1]\n",
    "    img_h = image.shape[0]\n",
    "\n",
    "    destination = np.float32([[img_w/2 - dst_size/2, img_h - bottom_offset],\n",
    "                  [img_w/2 + dst_size/2, img_h - bottom_offset],\n",
    "                  [img_w/2 + dst_size/2, img_h - dst_size - bottom_offset], \n",
    "                  [img_w/2 - dst_size/2, img_h - dst_size - bottom_offset],\n",
    "                  ])\n",
    "    \n",
    "    # 2) Apply perspective transform\n",
    "    # 3) Apply color threshold to identify navigable terrain/obstacles/rock samples\n",
    "    # 4) Convert thresholded image pixel values to rover-centric coords\n",
    "    # 5) Convert rover-centric pixel values to world coords\n",
    "    \n",
    "    navigable_x_world,navigable_y_world, threshed = get_navigible_terrain_world_coordinates(np.copy(img),\\\n",
    "                                                                                  source, \\\n",
    "                                                                                  destination,\\\n",
    "                                                                                  data.xpos[data.count],\\\n",
    "                                                                                  data.ypos[data.count],\\\n",
    "                                                                                  data.yaw[data.count],\\\n",
    "                                                                                  200,\\\n",
    "                                                                                  10)\n",
    "\n",
    "    rock_x_world, rock_y_world = get_rock_world_coordinates(np.copy(img),\\\n",
    "                                                          source, \\\n",
    "                                                          destination,\\\n",
    "                                                          data.xpos[data.count],\\\n",
    "                                                          data.ypos[data.count],\\\n",
    "                                                          data.yaw[data.count],\\\n",
    "                                                          200,\\\n",
    "                                                          10)\n",
    "    \n",
    "    obstacle_x_world, obstacle_y_world = get_obstacle_world_coordinates(np.copy(img),\\\n",
    "                                                                      source, \\\n",
    "                                                                      destination,\\\n",
    "                                                                      data.xpos[data.count],\\\n",
    "                                                                      data.ypos[data.count],\\\n",
    "                                                                      data.yaw[data.count],\\\n",
    "                                                                      200,\\\n",
    "                                                                      10)\n",
    "\n",
    "    \n",
    "    # 6) Update worldmap (to be displayed on right side of screen)\n",
    "    \n",
    "    if data.sentinel == 0:\n",
    "        data.map_count=np.zeros_like(data.worldmap)\n",
    "        data.sentinel =1\n",
    "        \n",
    "        \n",
    "    data.map_count[obstacle_y_world, obstacle_x_world, 0] += 1\n",
    "    data.map_count[navigable_y_world, navigable_x_world, 1] += 1\n",
    "    \n",
    "    obstacle = (data.map_count[:, :, 0] > data.map_count[:, :, 1])  &  (data.worldmap[:, :, 2] < 1)\n",
    "    naviagable = (data.map_count[:, :, 1] > data.map_count[:, :, 0]) &  (data.worldmap[:, :, 2] < 1)\n",
    "    data.worldmap[obstacle, 0] = 255\n",
    "    data.worldmap[obstacle, 1] = 0\n",
    "    data.worldmap[obstacle, 2] = 0\n",
    "\n",
    "    data.worldmap[naviagable, 0] = 0\n",
    "    data.worldmap[naviagable, 1] = 120\n",
    "    data.worldmap[naviagable, 2] = 0\n",
    "    \n",
    "    \n",
    "    #data.worldmap[obstacle_y_world, obstacle_x_world, 0] = 255\n",
    "    #data.worldmap[navigable_y_world, navigable_x_world, 0] = 0\n",
    "    \n",
    "    #data.worldmap[navigable_y_world, navigable_x_world, 1] =120\n",
    "    \n",
    "    data.worldmap[rock_y_world, rock_x_world, 2] = 255\n",
    "    data.worldmap[rock_y_world, rock_x_world, 0] = 0\n",
    "    data.worldmap[rock_y_world, rock_x_world, 1] = 0\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # 7) Make a mosaic image, below is some example code\n",
    "        # First create a blank image (can be whatever shape you like)\n",
    "    output_image = np.zeros((img.shape[0] + data.worldmap.shape[0], img.shape[1]*2, 3))\n",
    "        # Next you can populate regions of the image with various output\n",
    "        # Here I'm putting the original image in the upper left hand corner\n",
    "    output_image[0:img.shape[0], 0:img.shape[1]] = img\n",
    "\n",
    "        # Let's create more images to add to the mosaic, first a warped image\n",
    "    warped = perspect_transform(img, source, destination)\n",
    "        # Add the warped image in the upper right hand corner\n",
    "    output_image[0:img.shape[0], img.shape[1]:] = warped\n",
    "    \n",
    "    threshed[threshed>0]=255\n",
    "    output_image[img.shape[0]:img.shape[0]*2, img.shape[1]:,0] = threshed\n",
    "\n",
    "        # Overlay worldmap with ground truth map\n",
    "    map_add = cv2.addWeighted(data.worldmap, 1, data.ground_truth, 0.5, 0)\n",
    "        # Flip map overlay so y-axis points upward and add to output_image \n",
    "    output_image[img.shape[0]:, 0:data.worldmap.shape[1]] = np.flipud(map_add)\n",
    "\n",
    "\n",
    "        # Then putting some text over the image\n",
    "    cv2.putText(output_image,\"Testing\", (20, 20), \n",
    "                cv2.FONT_HERSHEY_COMPLEX, 0.4, (255, 255, 255), 1)\n",
    "    if data.count < len(data.images) - 1:\n",
    "        data.count += 1 # Keep track of the index in the Databucket()\n",
    "    \n",
    "    return output_image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define pathname to save the output video\n",
    "output = '../output/test_mapping.mp4'\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "data = Databucket() # Re-initialize data in case you're running this cell multiple times\n",
    "data.sentinel = 0\n",
    "clip = ImageSequenceClip(data.images, fps=60) # Note: output video will be sped up because \n",
    "                                          # recording rate in simulator is fps=25\n",
    "new_clip = clip.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time new_clip.write_videofile(output, audio=False)\n",
    "\n",
    "import io\n",
    "import base64\n",
    "video = io.open(output, 'r+b').read()\n",
    "encoded_video = base64.b64encode(video)\n",
    "from IPython.display import HTML\n",
    "HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded_video.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python [conda env:RoboND]",
   "language": "python",
   "name": "conda-env-RoboND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
